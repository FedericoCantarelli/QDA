{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "\n",
    "In a chemical process, three variables of interest are measured and monitored. The three variables are correlated to each other, and quality engineers would like to explore the possibility to reduce the dimensionality of the problem via PCA.\n",
    "1. How many PCs are needed to capture at least 90% of variability (use the correlation matrix).\n",
    "2. Compute the data reconstruction by using the first two PCs. \n",
    "3. Verify that using all the three PCs, the reconstructed data coincide with the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import the dataset\n",
    "data = pd.read_csv('ESE05_ex2.csv')\n",
    "\n",
    "# Print the first 5 rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1\n",
    "\n",
    "How many PCs are needed to capture at least 90% of variability (use the correlation matrix)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Solution\n",
    ">\n",
    "> Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation plot of the dataset\n",
    "pd.plotting.scatter_matrix(data, alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we want to apply the PCA on the correlation matrix, we need to standardize the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data by subtracting the mean and dividing by the standard deviation\n",
    "data_std = (data - data.mean()) / data.std()\n",
    "\n",
    "data_std.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's compute the covariance matrix on the standardized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix_std = data_std.cov()\n",
    "print(cov_matrix_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now compare it with the correlation matrix computed on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The two matrices are the same.\n",
    ">\n",
    "> Now perform the PCA on the standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the PCA object\n",
    "pca_std = PCA()\n",
    "# Fit the PCA object to the data\n",
    "pca_std.fit(data_std)\n",
    "# Print the eigenvalues\n",
    "print(\"Eigenvalues \\n\", pca_std.explained_variance_)\n",
    "# Print the eigenvectors\n",
    "print(\"\\nEigenvectors \\n\", pca_std.components_)\n",
    "# Print the explained variance ratio\n",
    "print(\"\\nExplained variance ratio \\n\", pca_std.explained_variance_ratio_)\n",
    "# Print the cumulative explained variance ratio\n",
    "print(\"\\nCumulative explained variance ratio \\n\", np.cumsum(pca_std.explained_variance_ratio_))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To capture at least 90% of variability, we need at least two PCs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the scree plot to compare the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the eigenvalues (scree plot)\n",
    "plt.plot(pca_std.explained_variance_, 'o-')\n",
    "plt.xlabel('Component number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Scree plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Plot the cumulated explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cumulative explained variance\n",
    "plt.plot(np.cumsum(pca_std.explained_variance_ratio_), 'o-')\n",
    "# add a bar chart to the plot\n",
    "plt.bar(range(0, len(pca_std.explained_variance_ratio_)), pca_std.explained_variance_ratio_, width = 0.5, alpha=0.5, align='center')\n",
    "plt.xlabel('Component number')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.title('Cumulative explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now plot the loadings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loadings\n",
    "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "ax[0].plot(pca_std.components_[0], 'o-')\n",
    "ax[0].set_title('Loading 1')\n",
    "ax[1].plot(pca_std.components_[1], 'o-')\n",
    "ax[1].set_title('Loading 2')\n",
    "ax[2].plot(pca_std.components_[2], 'o-')\n",
    "ax[2].set_title('Loading 3')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now compute the scores and plot the scatterplot of the scores along the first two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores\n",
    "scores = pca_std.transform(data_std)\n",
    "# create a dataframe with the scores\n",
    "scores_df = pd.DataFrame(scores, columns = ['z1', 'z2', 'z3'])\n",
    "# Print the first rows of the scores dataframe\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot with the first two scores\n",
    "plt.scatter(scores_df['z1'], scores_df['z2'])\n",
    "plt.xlabel('z1')\n",
    "plt.ylabel('z2')\n",
    "plt.title('Scatterplot of z1 vs z2')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2\n",
    "\n",
    "Compute the data reconstruction by using the first two PCs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Solution\n",
    "> \n",
    "> Data reconstruction based on the first $K=2$ PCs. \n",
    "> $$\\mathbf{\\hat{x}}_j^*(K) = z_{j1}\\mathbf{u_1} + z_{j2}\\mathbf{u_2}$$\n",
    "> for $j=1,\\ldots,n$.\n",
    ">\n",
    "> Remind that we are using the standardized data $\\mathbf{\\hat{x}}_j^*$. If we want to reconstruct the original data $\\mathbf{\\hat{x}}_j$, we need to multiply the reconstructed data by the standard deviation and add the mean.\n",
    "> $$\\mathbf{\\hat{x}}_j(K) = \\mathbf{\\hat{x}}_j^* s_i + \\bar{x}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's save in an array the mean and standard deviation of the original data\n",
    "mean = data.mean()\n",
    "std = data.std()\n",
    "\n",
    "# Compute the reconstructed data_std using the first two principal components\n",
    "reconstructed_data_std = scores_df[['z1', 'z2']].dot(pca_std.components_[0:2, :])\n",
    "# Now use the mean and standard deviation to compute the reconstructed data\n",
    "reconstructed_data = reconstructed_data_std.dot(np.diag(std)) + np.asarray(mean)\n",
    "\n",
    "# Compare the original data with the reconstructed data\n",
    "print(\"Original data\\n\", data.head())\n",
    "print(\"\\nReconstructed (ORIGINAL) data\\n\", reconstructed_data.head())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The reconstructed data are different from the original data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 3\n",
    "\n",
    "Verify that using all the three PCs, the reconstructed data coincide with the original data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Solution\n",
    ">\n",
    "> Data reconstruction based on all the PCs ($K=3$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the reconstructed data_std using the first two principal components\n",
    "reconstructed_data_std_3 = scores_df.dot(pca_std.components_)\n",
    "# Now use the mean and standard deviation to compute the reconstructed data\n",
    "reconstructed_data_3 = reconstructed_data_std_3.dot(np.diag(std)) + np.asarray(mean)\n",
    "\n",
    "# Compare the original data with the reconstructed data\n",
    "print(\"Original data\\n\", data.head())\n",
    "print(\"Reconstructed (ORIGINAL) data\\n\", reconstructed_data_3.head())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The reconstructed data coincide with the original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d717af6dd7bf581d6248419f276a5c41cb14dbf2eb68781a99eea88c8ccd834c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
