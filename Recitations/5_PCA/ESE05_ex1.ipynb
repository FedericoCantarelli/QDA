{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange\">EXERCISE CLASS 5 - Principal Component Analysis (PCA) </h1>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 1\n",
    "\n",
    "A pencil producer performs a process control by using 3 quality charcateristics. \n",
    "1. Pencil diameter\n",
    "2. Ultimate tensile strength\n",
    "3. The ease of sliding on paper (glide)\n",
    "\n",
    "Perform a Principal Component Analysis for these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Import the dataset\n",
    "data = pd.read_csv('ESE05_ex1.csv')\n",
    "\n",
    "# Print the first 5 rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reminder:\n",
    "> - Dataset $X$ is a $n \\times p$ matrix, where $n$ is the number of observations, and $p$ is the number of variables.\n",
    "> - The sample variance-covariance matrix $S$ is a $p \\times p$ matrix.\n",
    "> - The sample correlation matrix $R$ is a $p \\times p$ matrix.\n",
    ">\n",
    "> Eigendecomposition of $S$:\n",
    "> - Eigenvalues is a $p \\times 1$ vector, $\\lambda_1, \\lambda_2, \\dots, \\lambda_p$ (explained variance).\n",
    "> - Eigenvectors is a $p \\times p$ matrix, $\\mathbf{u_1}, \\mathbf{u_2}, \\dots, \\mathbf{u_p}$ (loadings).\n",
    "> $$ \\mathbf{u_1} = [u_{11}, u_{12}, \\dots, u_{1p}]^T$$\n",
    "> $$ \\mathbf{u_2} = [u_{21}, u_{22}, \\dots, u_{2p}]^T$$\n",
    "> $$ \\vdots$$\n",
    "> $$ \\mathbf{u_p} = [u_{p1}, u_{p2}, \\dots, u_{pp}]^T$$\n",
    "> - Projection of data onto the space spanned by the PCs (scores) ($n \\times p$ matrix):\n",
    "> $$ \\mathbf{z_1} = [z_{11}, z_{21}, \\dots, z_{n1}]^T = ( \\mathbf{X} - \\bar{ \\mathbf{X}}) \\mathbf{u_1}$$\n",
    "> $$ \\mathbf{z_2} = [z_{12}, z_{22}, \\dots, z_{n2}]^T = ( \\mathbf{X} - \\bar{ \\mathbf{X}}) \\mathbf{u_2}$$\n",
    "> $$ \\vdots$$\n",
    "> $$ \\mathbf{z_p} = [z_{1p}, z_{2p}, \\dots, z_{np}]^T = ( \\mathbf{X} - \\bar{ \\mathbf{X}}) \\mathbf{u_p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation plot of the dataset\n",
    "pd.plotting.scatter_matrix(data, alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can also compute the variance covariance matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variance covariance matrix using pandas\n",
    "cov_matrix = data.cov()\n",
    "print(cov_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can perform the principal component analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries for PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the PCA object\n",
    "pca = PCA()\n",
    "# Fit the PCA object to the data\n",
    "pca.fit(data)\n",
    "# Print the eigenvalues\n",
    "print(\"Eigenvalues \\n\", pca.explained_variance_)\n",
    "# Print the eigenvectors \n",
    "print(\"\\nEigenvectors \\n\", pca.components_)\n",
    "# Print the explained variance ratio\n",
    "print(\"\\nExplained variance ratio \\n\", pca.explained_variance_ratio_)\n",
    "# Print the cumulative explained variance ratio\n",
    "print(\"\\nCumulative explained variance ratio \\n\", np.cumsum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we retain the first two PCs we can explain about 97% of observed variability. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Projection of data onto the space spanned by the PCs (scores) ($n \\times p$ matrix):\n",
    "> $$ \\mathbf{z_1} = [z_{11}, z_{21}, \\dots, z_{n1}]^T = ( \\mathbf{X} - \\bar{ \\mathbf{X}}) \\mathbf{u_1}$$\n",
    "> $$ \\mathbf{z_2} = [z_{12}, z_{22}, \\dots, z_{n2}]^T = ( \\mathbf{X} - \\bar{ \\mathbf{X}}) \\mathbf{u_2}$$\n",
    "> $$ \\vdots$$\n",
    "> $$ \\mathbf{z_p} = [z_{1p}, z_{2p}, \\dots, z_{np}]^T = ( \\mathbf{X} - \\bar{ \\mathbf{X}}) \\mathbf{u_p}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For example, the first score (z1 projection of all data on 1 st PC):\n",
    "> $$ \\mathbf{z_1} = -0.8388 (\\mathbf{X_1} - \\mathbf{\\bar{x}_1}) - 0.354283 (\\mathbf{X_2} - \\mathbf{\\bar{x}_2}) - 0.413237 (\\mathbf{X_3} - \\mathbf{\\bar{x}_3})$$\n",
    "> In this case, $\\mathbf{\\bar{x}} = [0, 0, 0]^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores (i.e. all the principal components, n x 3) \n",
    "scores = pca.transform(data)\n",
    "# create a dataframe with the scores\n",
    "scores_df = pd.DataFrame(scores, columns = ['z1', 'z2', 'z3'])\n",
    "# Print the first rows of the scores dataframe\n",
    "scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scores in a scatter plot\n",
    "pd.plotting.scatter_matrix(scores_df, alpha = 1)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now we can plot the loadings $\\mathbf{u_1}$, $\\mathbf{u_2}$, $\\mathbf{u_3}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loadings\n",
    "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "ax[0].plot(pca.components_[0], 'o-')\n",
    "ax[0].set_title('Loading 1')\n",
    "ax[1].plot(pca.components_[1], 'o-')\n",
    "ax[1].set_title('Loading 2')\n",
    "ax[2].plot(pca.components_[2], 'o-')\n",
    "ax[2].set_title('Loading 3')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1.1\n",
    "\n",
    "By using the data of Exercise 1: \n",
    "1. Compute the sample correlation matrix.\n",
    "2. Compute the sample variance covariance matrix of standardised variables and compare the result with the result of point 1. \n",
    "3. Apply the PCA by using the correlation matrix and compare the results with the ones obtained applying the PCA to the variance covariance matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 1\n",
    "\n",
    "Compute the sample correlation matrix."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix of the scores\n",
    "corr_matrix = data.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparison between correlation matrix and variance covariance matrix of input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cov_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 2\n",
    "\n",
    "Compute the sample variance covariance matrix of standardised variables and compare the result with the result of point 1. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Solution\n",
    ">\n",
    "> Let's standardise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data by subtracting the mean and dividing by the standard deviation\n",
    "data_std = (data - data.mean()) / data.std()\n",
    "\n",
    "data_std.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compute the variance covariance matrix of standardised variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix_std = data_std.cov()\n",
    "print(cov_matrix_std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now compare it with the original correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corr_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The sample correlation matrix coincides with the sample variance covariance matrix of standardized variables!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point 3\n",
    "Apply the PCA by using the correlation matrix and compare the results with the ones obtained applying the PCA to the variance covariance matrix. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Solution\n",
    "> \n",
    "> Apply the PCA on the correlation matrix of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the PCA on the correlation matrix instead of the covariance matrix\n",
    "pca_corr = PCA()\n",
    "pca_corr.fit(data_std) \n",
    "# Compare the eigenvalues\n",
    "print(\"Eigenvalues from ORIGINAL data \\n\", pca.explained_variance_)\n",
    "print(\"Eigenvalues from STANDARDIZED data \\n\", pca_corr.explained_variance_)\n",
    "# Compare the eigenvectors\n",
    "print(\"\\nEigenvectors from ORIGINAL data \\n\", pca.components_)\n",
    "print(\"Eigenvectors from STANDARDIZED data \\n\", pca_corr.components_)\n",
    "# Compare the explained variance ratio\n",
    "print(\"\\nExplained variance ratio from ORIGINAL data \\n\", pca.explained_variance_ratio_)\n",
    "print(\"Explained variance ratio from STANDARDIZED data \\n\", pca_corr.explained_variance_ratio_)\n",
    "# Compare the cumulative explained variance ratio\n",
    "print(\"\\nCumulative explained variance ratio from ORIGINAL data \\n\", np.cumsum(pca.explained_variance_ratio_))\n",
    "print(\"Cumulative explained variance ratio from STANDARDIZED data \\n\", np.cumsum(pca_corr.explained_variance_ratio_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Different PCs**\n",
    ">\n",
    "> The eigenvalues and eigenvectors of the correlation matrix have no simple relationship with those of the corresponding covariance matrix. \n",
    ">\n",
    "> The PCs for correlation and covariance matrices do not, therefore, give equivalent information, nor can they be derived directly from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loadings of original and standardized data\n",
    "fig, ax = plt.subplots(2, 3, figsize = (15, 5))\n",
    "ax[0, 0].plot(pca.components_[0], 'o-')\n",
    "ax[0, 0].set_title('Loading 1, original data')\n",
    "ax[0, 1].plot(pca.components_[1], 'o-')\n",
    "ax[0, 1].set_title('Loading 2, original data')\n",
    "ax[0, 2].plot(pca.components_[2], 'o-')\n",
    "ax[0, 2].set_title('Loading 3, original data') \n",
    "ax[1, 0].plot(pca_corr.components_[0], 'o-')\n",
    "ax[1, 0].set_title('Loading 1, standardized data')\n",
    "ax[1, 1].plot(pca_corr.components_[1], 'o-')\n",
    "ax[1, 1].set_title('Loading 2, standardized data')\n",
    "ax[1, 2].plot(pca_corr.components_[2], 'o-')\n",
    "ax[1, 2].set_title('Loading 3, standardized data')\n",
    "fig.subplots_adjust(hspace = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how the value of the first loading $u_{11}$ was the one with the largest variance when we applied the PCA to the original (non standardised) data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A few considerations: \n",
    "> - If the input variables are on different scales/units, PCA on standardized data should be applied to avoid undesired effects, i.e., scale effects, on individual variances.\n",
    "> - If there are large differences between the variances of the elements of $\\mathbf{X}$, then those variables whose variances are largest will tend to dominate the first few PCs. \n",
    "> - If the input variables are in the same scale with similar variance, both PCA on original and standardized data may be applied, but they will yield different results. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d717af6dd7bf581d6248419f276a5c41cb14dbf2eb68781a99eea88c8ccd834c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
